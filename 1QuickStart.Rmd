---
title: "1 Quickstart"
author: "Leona Odole"
date: '2024-02-08'
output: 
  rmdformats::html_clean
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, fig.align  = "center")
```

# 1. Quickstart: Amortized Poserior Estimation

Welcome to Bayesflow with R. This tutorial follows the same steps as the original Python based tutorial with the additional quirks of using a Python package within R. 

## 1.1 Reticulate Library

Using Python packages in R is relatively simple. The [reticulate](https://rstudio.github.io/reticulate/) package allows users to use Python packages directly from R, with more detailed instructions on how its used found on their github link above. Before starting, bayesflow needs to be installed following the installation instructions and there should be a virtual environment within which bayesflow is installed. 

As with any other package it can be installed directly from CRAN using the following:

```{r eval=FALSE}
install.packages("reticulate")
```

Once `reticulate` is installed the next step is setting up the bayesflow to be callable from R. First call the library to the R session. 

```{r}
library(reticulate)
```

Next point reticulate to the correct virtual environment where bayesflow is stored. 

```{r echo = FALSE}
use_condaenv("/Users/eodole/miniforge3/envs/bayes3")
```

```{r eval = FALSE}
use_condaenv("/path_to_environment/environment_name")
```


Then call the bayesflow module into the R session and give it an alias

```{r}
bf <- import("bayesflow")
```


Then we can set a local seed for reproducibility, although the results will not necessarily be the same as in Python. 

```{r}
set.seed(2023)
```

## Libraries 
For your convenience all additional  R libraries are loaded here, and then mentioned again specifically where they are used during the tutorial.

```{r}
library(MASS)
library(ggplot2)
library(zoo)
library(GGally)
library(tidyr)
library(ggpubr)
library(dplyr)
```


## 1.2 Introduction to BayesFlow 

In this notebook, we will estimate the means of a multivariate Gaussian model and illustrate some features of the library along the way. Above, we have already imported the core functionality we will need for this notebook. In brief: 

- The module `simulation` contains high-level wrappers for gluing together priors, simulators and context generators into a single `GenerativeModel` object, which will generate all quantities of interest for a modeling scenario.
- The modules `networks` contains the core neural architectures used for various tasks, e.g., an `InvariantNetwork` for realizing [normalizing flows](https://paperswithcode.com/method/normalizing-flows) or a `DeepSet` for learning permutation-invariant summary representations (embeddings). 
- The module `amortizers` contains high-level wrappers which connect the variaous networks together and instruct them about their particular goals in the inference pipeline. 
- The module `trainers` contains high-level wrappers for dictating and the _training phase_ of an amortized posterior. Typically, the standard `Trainer` will take care of most scenarios. 

The nuts and bolts of using BayesFlow for Bayesian parameter estimation have already been described in the corresponding papers: 

- Radev, S. T., Mertens, U. K., Voss, A., Ardizzone, L., Köthe, U. (2020). BayesFlow: Learning complex stochastic models with invertible neural networks. _IEEE Transactions on Neural Networks and Learning Systems, 33(4)_, 1452-1466.
- Radev, S. T., Graw, F., Chen, S., Mutters, N. T., Eichel, V. M., Bärnighausen, T., & Köthe, U. (2021). OutbreakFlow: Model-based Bayesian inference of disease outbreak dynamics with invertible neural networks and its application to the COVID-19 pandemics in Germany. _PLoS computational biology, 17(10)_, e1009472.
- Schmitt, M., Bürkner, P. C., Köthe, U., & Radev, S. T. (2021). Detecting model misspecification in amortized Bayesian inference with neural networks. _arXiv preprint arXiv:2112.08866_.

At the high level, out architecture consists of summary network $h$ and an interence network $f$ which jointly amortize a generative model. The summary network transforms input data $x$ of potentially variable size to a fixed-length representations. The inference network generates random draws from an approximate $q$ via a conditional invertible neural network (cINN). The process is illustrated in the figure below: 

![BayesFlow Overview](/Users/eodole/Desktop/HiWi Stelle/Images/bayesflow_overview.png)

The left panel illustrates the _training phase_. During this phase, only the model (i.e., simulator and prior) is used to jointly train the summary and inference networks. The right panel illustrates the _inference phase._ During this phase, arbitrarily many actually observed data sets can be fed through the networks to obtain posteriors. For instance, in one recent paper [(https://www.nature.com/articles/s41562-021-01282-7)](https://www.nature.com/articles/s41562-021-01282-7), the authors applied pre-trained networks to more than one million observed data sets! Now let’s get into some coding…


## 1.3 Defining the Generative Model

From the perspective of the BayesFlow framework, a generative model is more than just a prior and a simulator. In addition, it consists of various _implicit_ context assumptions, which we can make _explicit_ at any time. Furthermore, we can also _amortize_ over these context variables, thus making our real-world inference more flexible (i.e., applicable to more contexts). The figure below illustrates the skeleton of a generative model as conceptualized in the BayesFlow framework.

![Generative Model](/Users/eodole/Desktop/HiWi Stelle/Images/generative_model.png)

The conceptual model allows you to tackle very flexible model families with BayesFlow, as well as various other Bayesian tasks, such as prior sensitivity analysis or multiverse analysis. 

### 1.3.1 Prior 

The toy Gaussian Model we will use for this tutorial is of the form
$$ \mu \sim N_D(0, \sigma_0 \mathbb{I}) $$ 
$$ x_n \sim N(\mu, \sigma_1 \mathbb{I}) \text{ for } n = 1,..,N $$
For simplicity we set $\sigma_0 = \sigma_1 = 1$.

A package for simulating multidimensional normal distributions is `MASS`, however the center and spread parameters need to be explicitly given. After loading the appropriate library we can define the prior function to simulate single draws from the prior distribution of $\mu$. 


```{r}
#library(MASS)

prior_fun <- function(D=4){
  mu <- rep(0,D)
  Sigma <- diag(D)
  return(mvrnorm(1,mu,Sigma))
}
```

Integrating into bayesflow we can pass our prior function into the wrapper function `Prior` . In testing it was found that best practice is to explicitly typecast functions from one language to another when using wrappers, which can be done with the reticulate funciton `r_to_py()` similarly the reverse direction typecast is `py_to_r()`.

```{r}
prior <- bf$simulation$Prior(prior_fun = r_to_py(prior_fun))
```

A difference between the native Python and R is how information is stored. When working with bayesflow through reticulate, there tends to be errors related to typecasting. Best practice is to explicitly declare variable types or require that variables are converted appropriately before being passed to Python based functions.  When calling the prior object to for batching the following happens

```{r error=TRUE, warning=TRUE, message=TRUE}
prior(batch_size = 10)
```

Instead by explicitly declating that 10 is in fact an integer, the prior exhibits the appropriate behavior. 

```{r}
prior(batch_size = as.integer(10))
```

Below is a handy conversion guide from Posit

![Link to posit](/Users/eodole/Desktop/HiWi Stelle/Software/R/rcheatsheet.png)

[Posit Quick Guide](https://raw.githubusercontent.com/rstudio/cheatsheets/main/reticulate.pdf)

The `Prior` function natively returns a dictionary which is treated as a list in R. The prior also generated information that we never specified in a list. This needs some explanation. In the Generative Model diagram, we can see that there is also an option to provide context variables which modify the behavior of the prior, whenever it is desireable. We will see this when we illustrate how to perform _prior sensitivity_ analysis. We also see two types of context variables in the prior output. The interface distinguishes between two types of context: `batchable_context` and `non_batchable_context`. The distinction is a purely technical rather than a conceptual one: 
- Batchable context variables differ for each simulation  in each training batch of simultions
- Non-batchable context variables stay the same for each simulation in a batch, but differ across simulated batches

Examples for __batchable__ context variables include experimental design variables, design matrices, etc. Examples for __non-batchable__ context variables include the number of observations in an experiment, positional encodings, time indices, etc. While the latter can also be considered batchable in principle, batching them would require non-Tensor (i.e., non-rectangular) data structures, which usually means inefficient computations


### 1.3.2 Simulator

As with the prior function, anytime that bayesflow is interacting with R, it is important to have the object types matching with the function they're interacting with. Typically the next step would be to define a likelihood function, which will take the parameters of generative by our prior and generate observations from the model. However as we're working across programming language it is important to have an overview of how the different `BayesFlow` wrappers interact with their R-based counterparts. The following diagram illustrates at a high level how the `Prior`, likelihood function (`Simulator`) and `GenerativeModel` interact with one another.  

![Diagram of How R & Python Interface](/Users/eodole/Desktop/HiWi Stelle/Software/R/interaction.png)

The function `Generative Model()` passes the results of the prior function to the simulator, however, this is done within Python meaning that the pass directly from Python to the simulator function. 

In the diagram above, the the paths are represented by the following code segments. 

The arrows representing the black are how the functions would normally interact in Python, the sample code can run directly in Python.
```
def prior_fun():
  # code for the prior function 
  return prior_results

def likelihood_fun(params, n_samples = 50): 
  # code for likelihood function 
  return likelihood_results

prior = bf.simulation.Prior(prior_fun=prior_fun)

simulator = bf.simulation.Simulator(simulator_fun=likelihood_fun)

model = bf.simulation.GenerativeModel(prior=prior, simulator=simulator)

model(batch_size=3)

```



In this case, our simulator function is equally simple to our prior function. We will call it a likelihood function, in correspondence with standard Bayesian terminology, and pass it to the `Simulator` wrapper.

Note, that we define our `simulator_fun` with two arguments. A positional argument which stands for a single random draw from the prior and a keyword argument `n_obs` which represents the number of observations $N$ we will generate from the likelihood for each draw from the prior. As some point, we want to vary $N$ during training, so that the architecture can generalize to different $N$ during inference.

The above code concept would function as expected, running the model three times and giving results in line with the simulation. In our case this would result in three batches of 50 observations from a four-dimensional multivariate normal. 

However as indicated in the diagram using R with the BayesFlow package means going back and forth between the two programming languages. Problems can arise if the inputs for a function in one language are in the format of another language. Therefore it is important to ensure that each function is receiving the input formatted in the language where the function lives. 

The following code segment illustrates the path including 6a where the function `simulator_fun` is given the improper format and results in errors. 
#### Error 1: Broadcasting 
```{r error=TRUE}
# Prior Function as defined in R 
prior_funR <- function(D=4){
  mu <- rep(0,D)
  Sigma <- diag(D)
  # The results are in the R format
  return(mvrnorm(1,mu,Sigma))
}


# Here results from prior_funR() are passed through r_to_py() to reformat them for 
# the priorPy function
priorPy <- bf$simulation$Prior(prior_fun = r_to_py(prior_fun))


# This likelihood function, which is native to R, assumes that it will receive all arguments 
# in R format, however the argument 'params' which is passed direction from the GenerativeModel() 
# function are in the Python format. Since n_obs has a default argument, as long as it
# is not specified in the GenerativeModel() it will also be in the proper format for the 
# likelihood function. 
likelihood_fun <- function(params, n_obs = 50){
  #6a
  D <- length(params) # error here 
  Sigma <- diag(D)
  return(mvrnorm(n=n_obs, mu=params, Sigma = Sigma )) # error here
}


#Here the results from likelihood_fun() are again passed through r_to_py() to reformat them
simulator <- bf$simulation$Simulator(simulator_fun = r_to_py(likelihood_fun))


# Assigning GenerativeModel() to the variable model, causes it to run then save the results to 
# the variable 'model' in the R Global Environment
model <- bf$simulation$GenerativeModel(prior = prior, simulator = simulator)

```


The error was caused because of step 6a on the diagram on which the intermediate results are passed between R and Python without being properly formatted. Instead by properly formatting the results as in the following code snippet we get, 

#### Error 2: Non-conformable arrays

```{r error=TRUE}
# Prior Function as defined in R 
prior_funR <- function(D=4){
  mu <- rep(0,D)
  Sigma <- diag(D)
  # The results are in the R format
  return(mvrnorm(1,mu,Sigma))
}


# Here results from prior_funR() are passed through r_to_py() to reformat them for 
# the priorPy function
priorPy <- bf$simulation$Prior(prior_fun = r_to_py(prior_fun))


# Here we change how the intermediate results from PriorPy are used within the likelihood function
likelihood_fun <- function(params, n_obs = 50){
  # Step 6b: The results that were passed from Python are first converted in the proper format
 
  mu <- py_to_r(params)
  D <- length(params)
  Sigma <- diag(D)
  
  return(mvrnorm(n=n_obs, mu=mu, Sigma = Sigma )) 
}


#Here the results from likelihood_fun() are again passed through r_to_py() to reformat them
simulator <- bf$simulation$Simulator(simulator_fun = r_to_py(likelihood_fun))


# Assigning GenerativeModel() to the variable model, causes it to run then save the results to 
# the variable 'model' in the R Global Environment
model <- bf$simulation$GenerativeModel(prior = prior, simulator = simulator)


```

Here we can see that although we have done the appropriate type conversion, there is still an error. Through additional testing I found that the `py_to_r()` function does not always perform type conversions as expected. Although Python passes the `params` as an array and `r_to_py()` should convert this to a vector, when tested the object returned while numeric is not a vector and needed to be typecast explicitly. In general there may be many type related errors so in the event of errors it is best practice to make sure that variables are being passed to functions in the form they expect. 

#### Correct Way

```{r}


likelihood_fun <- function(params, n_obs = 50){
  # Step 6b: Also from further testing we discovered that the py_to_r type conversion does not
  # always behave as expected, but by explicitly typecasting to a vector, which is what 
  # mvrnorm is expecting you can make sure that there are no type related errors
  mu <- as.vector(py_to_r(params)) 
  
  # As additional good practice we could add while adding additional functions to ensure they're in 
  # the proper format 
  ##  n_obs = py_to_r(n_obs)
  
  D <- length(mu)
  
  Sigma <- diag(D)
  
  return(mvrnorm(n=n_obs, mu=mu, Sigma = Sigma )) 
}


#Here the results from likelihood_fun() are again passed through r_to_py() to reformat them
simulator <- bf$simulation$Simulator(simulator_fun = r_to_py(likelihood_fun))


# Assigning GenerativeModel() to the variable model, causes it to run then save the results to 
# the variable 'model' in the R Global Environment
model <- bf$simulation$GenerativeModel(prior = prior, simulator = simulator)


```


Once we have the prior and likelihood functions running smoothly, the generative model will provide an internal consistency check and report on the tensor shape of the model output. We can also manually inspect its outputs for `batch_size = 3` (i.e., three simulations): 


```{r}
out <- model(batch_size = as.integer(3))
```


```{r}
names(out)
```

```{r}
dim(out$sim_data)
```

The output of the Generative Model is also a Python `dict` converted to R `list` with many different elements. The names of the elements should given you some intuition of what they represent, namely, the different types of context variables (none in this example) for prior and simulator. With this simple set-up, we can now proceed to do some posterior estimation. 

## 1.4 Defining the Neural Approximator

### 1.4.1 Summary Network

Since our likelihood generates data exchangeably, we need to respect the permutation invariance of the data. For that, we will use a `DeepSet` which does exactly that. This network will take (at least) 3D tensors of shape `(batch_size, n_obs, D)` and reduce them to 2D tensors of shape `(batch_size, summary_dim)`, where `summary_dim` is a hyperparameter to be set by the user (you). Heuristically, this number should not be lower than the number of parameters in a model. Below, we create a permutation-invariant network with `summary_dim = 10`:

```{r}
summary_net <- bf$networks$DeepSet(summary_dim=as.integer(10))
```


Note that the hyperparameter setting for the `InvariantNetwork` are all provided inside a single Python dictionary, which means ensuring the proper variable type. 

It helps to inspect the output of the summary network manually and confirm its operation: 

```{r}
test_input <- model(batch_size = as.integer(3))
summary_rep <- summary_net(r_to_py(test_input$sim_data))

print("Size of Simulated Data Sets:")
dim(test_input$sim_data)
print("Summary Vectors:")
summary_rep
```





The summary vectores are then entered as conditions for the inference network. Upon convergence of the simulation based training, we can think of a them as a learned *summary statistics* or *data embeddings*.

### 1.4.2 Inference Network

Next we define the main workhorse of our our framework for amortized posterior inference - the conditional invertible neural network (cINN). The only mandatory hyperparameter for the `InvertibleNetwork` is the number of parameters we aim to estimate, in our case `num_params = 4`. However, we can change some more, for instance set the number of coupling layers `num_coupling_layers = 4`, which will make training a bit faster than using the default `num_coupling_layers = 6`, but also reduce the expressiveness (performance) of our network. Naturally, we don’t need a lot of expressiveness for our trivial Gaussian model, so we can proceed with `num_coupling_layers = 4`.

The invertible inference network has the following further hyperparameters:

* `num_params` (mandatory) - The number of model parameters (eq. the dimensionality of the latent space).

* `num_coupling_layers` - The number of invertible layers. The more layers, the more powerful the network, but the slower and possibly less stable the training. Typically 6-10 coupling layers should be sufficient.

* `coupling_settings` - The settings for the internal coupling layers. Typically, the defaults work well. For online training, you should switch off the default regularization as it may prevent optimal performance.

* `coupling_design` - You could try setting the design to spline for more superior performance on lower-dimensional problems.

* `soft_clamping` - The soft-clamping parameter. Just use the default value.

* `permutation` - Whether to use permutations before each coupling layer. Should be used by default.

* use_act_norm - Whether to apply activation normalization after each [coupling layer](https://arxiv.org/abs/1807.03039) works well in practice and stabilizes training.

* `act_norm_init` - In some cases, you can perform data-dependend initialization of the ActNorm layers, as in [this paper](https://arxiv.org/abs/1807.03039).

* `use_soft_flow` - Whether to use a [SoftFlow architecture](https://arxiv.org/abs/2006.04604) which is useful for degenerate distributions.

* `soft_flow_bounds` - The bounds for the varying standard deviation of SoftFlow’s noise. Do not touch, unless you have good reasons to.

You can glean all the defaults in the default_settings module. For most applications, you only need to define the num_params and num_coupling_layers hyperparameters.

Note, that we also remove the default `L2` and `dropout` regularization from the networks, as we need this only for offline learning with pre-simulated data.

```{r}
inference_net <- bf$networks$InvertibleNetwork(
  num_params = as.integer(4),
  num_coupling_layers = as.integer(4), 
  coupling_settings = r_to_py ( #variable type
    # We need a Python dictionary which is equivalent to a named list
    list("dense_args"= list("kernel_regularizer" = NULL), "dropout" = FALSE))
)
```

Again, we can inspect the raw outputs of the cINN by feeding it the parameter draws and corresponding data summaries. This network is slightly more involved than the summary network, as it has two mandatory inputs: targets and condition. It also has two outputs: `z` and `log_det_J`, which represent the latent representation of the parameters and the log of the Jacobian determinant, respectively. Inspecting the outputs we get:

```{r}
cINN_outputs <- inference_net(r_to_py(test_input$prior_draws),summary_rep)
print("Latent variables z:")
cINN_outputs[[1]]
print("Log det of Jacobian:")
cINN_outputs[[2]]
```


### 1.4.3 Amortized Posterior

We can now connect the `summary_net` and the `inference_net` via the high-level wrapper `AmortizedPosterior`. This wrapper knows how to compute its loss function, draw samples from the approximate posterior given new data and also compute normalized posterior densities.

```{r}
amortizer <- bf$amortizers$AmortizedPosterior(inference_net, summary_net)
```



## 1.5 Defining the Trainer 

The `Trainer` instance connects a generative model with an amortizer and enables various types of simulation-based training. Actually, it has only a single mandatory argument, `amortizer`, which expect an `Amortized*` instance. However, in order to be able to perform on-the-fly simulation-based training (see below), we also need to provide the generative model. Note, that the generative model does not need to use our provided wrappers, but the keys of its dictionary output should adhere to BayesFlow’s expectations.

Note: If you want to automatically save the `amortizer` and related loss history, you can provide a `checkpoint_path` argument indicating the folder for storing the checkpoints.

```{r}
trainer <- bf$trainers$Trainer(amortizer = amortizer,generative_model = model)
```

Actually, a `Trainer` instance does a little more than connect a generative model to an amortizer. It does so through the help of a configurator. In our case the `configurator` was inferred from the type of amortizer provided, but for more involved models, you should define the configurator explicitly.

What does a configurator do? It takes the raw outputs of the generative models and turns them into something with which neural networks can work:

![Trainer Connection Diagram](/Users/eodole/Desktop/HiWi Stelle/Images/trainer_connection.png)

Let's see how this actually works by accessing the default (inferred) configurator from the `Trainer` instance. 

```{r}
# Simulate some data again
out <- model(as.integer(3))

print("Keys of Simulated Data List (dict):")
print(names(out))
```

```{r}
conf_out <- trainer$configurator(out)
names(conf_out)
```

The default configurator for posterior inference differentiates between three types of model outputs:

1. `parameters` - these are the quantities for which we want posteriors.

2. `summary_conditions` - these are the quantities that go through the summary network (typically the raw data).

3. `direct_conditions` – these are concatenated with the outputs of the summary network and passed directly to the inference network.

In our case, `summary_conditions` simply correspond to the data, and parameters correspond to the prior draws, but you can imagine that more complex scenarios are possible. Let’s confirm that indeed the above is true and that the configurator parameters and summary conditions are relatively close to their counterparts. 

```{r}
# Sanity Check that the values are close together 
all(abs(out$sim_data - conf_out$summary_conditions) < 1e-05)

# Python Equivalent: 
# np.allclose(out["sim_data"], conf_out["summary_conditions"])
```

```{r}
# Sanity Check that the values are close together 
all(abs(out$prior_draws - conf_out$parameters) < 1e-05)

# Python Equivalent: 
# np.allclose(out["prior_draws"], conf_out["parameters"])
```

Here, we are not using direct equality, since the configurator converts `float64` numbers to `float32` in order to use GPU more efficently. 

## 1.6 Training Phase 

The following training modes are currently available:

* __Online training__ - This training regime is optimal for fast generative models which can efficiently simulate data on-the-fly. In order for this training regime to be efficient, on-the-fly batch simulations should not take longer than 2-3 seconds. The networks never see the same simulations twice, so no regularization is necessary.

* __Experience replay__ - This training regime is also good for fast generative models which can efficiently simulated data on-the-fly. It will use a memory replay buffer, as utilized in reinforcement learning, so the network will eventually “experience” some simulations multiple times.

* __Round-based training__ - This training regime is optimal for slow, but still reasonably performant generative models. In order for this training regime to be efficient, on-the-fly batch simulations should not take longer than one 2-3 minutes.

* __Offline training__ - This training regime is optimal for very slow, external simulators, which take several minutes for a single simulation. It assumes that all training data has already been simulated and stored on disk. Default regularization should be used (or even increase for very small data sets).

Usually, domain modelers have a pretty good understanding of how fast a simulation model runs. We can also quickly measure the time taken for a given number of simulations (`batch_size`) directly inside the notebook.

```{r}
system.time({model(as.integer(32))})
```

Note that in comparison to the native Python, running `bayesflow` in R is considerably slower, with the equivalent Python yeilding

![Python Timing](/Users/eodole/Desktop/HiWi Stelle/Images/Python_timing.png)

As the funciton `system.time` is measured in second we can see that the elapsed Python function needing only $0.00123$ seconds is considerably faster, this is likely due to using reticulate and the repeated need for type conversions. 

However, we are still well below the 2-3 seconds for online training, so that is what we will do. Online training has three mandatory parameters:`epochs`, `iterations_per_epoch`, and `batch_size`. Thus, the total number of simulations that will be performed by a single call (run) will be 
$$epochs  * iterations\_per\_epoch  * batch\_size$$
Moreover, the networks will never experience the same simulation twice, since each batch will contain a new set of simulations.

### 1.6.1 Online Training

Note how the average loss goes down, along with the learning rate (LR). The latter happens, because BayesFlow uses a cosine decay for the learning rate by default, unless a custom optimizer from `tensorflow.optimizers` is provided, which has not yet been tested with R reticulate. Thus, the learning rate will decrease atuomatically from its default value of $0.0005$  
to $0$ over the course of the training. We will also use $200$ simulations for tracking validation error (even though we don’t strictly need to do this for online learning, where the network sees a completely new batch of data in each iteration).

Depending on the hardware, this training should take between $30$ seconds and $5$ minutes. Note, that for actual applications, we will train considerably more than 10 epochs. 

```{r}
history <- trainer$train_online(epochs = as.integer(10), iterations_per_epoch = as.integer(1000), batch_size = as.integer(32), validation_sims = as.integer(200))
```


### 1.6.2 Inspecting the Loss

BayesFlow provides a utility function `plot_losses` from the `diagnostics` module, that also us to inspect the evolution of the loss.

```{r}
bf$diagnostics$plot_losses(history$train_losses, history$val_losses)
```

However this does not work natively in Rmarkdown. Instead we can recreate this plot in ggplot2. First we want to verify that we have all the data from training
```{r}
dim(history$train_losses)
```
Here we have the loss for every iteration and 
```{r}
dim(history$val_losses)
```
the validation loss for every epoch. 

```{r warning=FALSE}
library(ggplot2)
library(zoo)  # Needed for moving average but alteravtives include: TTR, data.table, RcppRoll, tidyquant

ma_window <- 0.01*nrow(history$train_losses)

ggplot(data=history$train_losses, aes(x = as.numeric(row.names(history$train_losses)),  y=Default.Loss))+
  # Training Loss Trend Line
  geom_line(alpha=0.9, aes(color='Training'))+
  # Moving Average of Training 
  geom_line(aes(y=zoo::rollmean(Default.Loss, k=ma_window,na.pad = TRUE), color= "Training (Moving Average)")) +
  # Validation Trend
  geom_line(data = history$val_losses, aes(x = as.numeric(row.names(history$val_losses))*1000, y = Default.Loss , color= "Validation"), linetype = 2) +  
  # Validation Points 
  geom_point(data = history$val_losses, aes(x = as.numeric(row.names(history$val_losses))*1000, y = Default.Loss , color= "Validation"), size=1)+
  # Color Scheme
  scale_color_manual(values=c("maroon4", "grey", "black"))+ 
  # Labels 
  labs(x = "Training Step #", y = "Loss Value", color = "Key") + 
  theme(legend.position = c(0.75,0.75))
```

### 1.6.3 Validating Consistency 

Validating the consistency of our model-amortizer coupling is an important step which should be performed before any real data are presented to the networks. In other words, the model should work in the ‘’small world’’, before going out in the world of real data. In addition to a smooth loss reduction curve, we can use at least four handy diagnostic utilities.

For a better illustration, we will start by generating some test simulations (not seen during training). Note, that we also use the default configurator to prepare these test simulations for interacting with the networks.

```{r}
test_sims <- trainer$configurator(model(as.integer(500)))
```

#### 1.6.3.1 Latent Space Inspection

Since our training objective prescribes a unit Gaussian to the latent variable $z$, (see: [https://arxiv.org/abs/2003.06281](https://arxiv.org/abs/2003.06281)), we expect that, upon good convergence, the latent space will exhibit the prescribed probabilistic structure. 

```{r}
z <- amortizer(test_sims)
print("Z-samples")
z[[1]]
```
Unfortunately, the amortiizer returns a TensorFlow tensor, which reticulate is unable to convert. Although up until this point we've been able to avoid using any additional Python modules, we now need numpy in order to convert the tf. Tensor into an R-useable format. Recall that in Python the equivalent to a dataframe is a pandas dataframe. Conveniently, import the `pandas` package and use package functions for the conversion.

```{r}
pd <- import("pandas")
```


```{r}
 z_samples <- pd$DataFrame(z[[1]])
head(z_samples)
```

Checking against our results from above we can see that the dataframe is accurate and was converted as expected. Next we can move on to creating a plot similar that produces by the bayesflow function `bf.diagnostics.plot_latent_space_2d`. For this we need the library `GGally`, an extension of `ggplot2 ` that allows us to create nice correlation


```{r}
library(GGally)
```

```{r, warning=FALSE}
ggpairs(z_samples,
        xAxisLabels = c("Latent Dim. 1","Latent Dim. 2","Latent Dim. 3","Latent Dim. 4"), 
        yAxisLabels = c("Latent Dim. 1","Latent Dim. 2","Latent Dim. 3","Latent Dim. 4"), 
        upper = list(continuous = wrap("points", alpha =0.3, color = "maroon4")),
        diag = list(continuous = wrap("barDiag", fill = "maroon4")),
        lower = list(continuous = wrap("density",color = "maroon4" ))
        ) +
  xlab("Latent Dimension") + 
  ylab("Latent Dimension") + 
  theme_bw()
  
  
```


#### 1.6.3.2 Inspecting the Loss

By now a classic in Bayesian analysis. If you are not familiar with this procedure, you must read about it here: https://arxiv.org/abs/1804.06788

SBC is a technique used to assess whether a probabilistic model correctly infers parameters from data. The basic idea is to simulate a large number of datasets from the model’s prior distribution and then perform posterior inference on these simulated datasets. The goal is to check if the inferred posterior distributions are consistent with the priors. Essentially, SBC tests if the model can accurately recover the parameters it used to generate the data in the first place. This process helps identify any systematic biases or inaccuracies in the model’s inference process.

To perform SBC, we first need to obtain `L` number of posterior draws from `M` simulated data sets. While the procedure is typically intractable, amortized inference allows us to perform SBC instantly.

```{r}
# Obtain 100 posterior samples for each simulated data set in test_sims
posterior_samples = amortizer$sample(test_sims, n_samples = as.integer(100))

```

Here we can recreate the Python function `bf.diagnostics.plot_sbc_histograms` which creates and plots publication-ready histograms of rank statistics for simulation-based calibration (SBC) checks according to [this paper](https://arxiv.org/abs/1804.06788) from Talts et al. (2018). 

```{r}
# Determine the ratio of simulations to prior draws
n_sim <- dim(posterior_samples)[1]
n_draws <- dim(posterior_samples)[2]
n_params <- dim(posterior_samples)[3]
dim(posterior_samples)
```

```{r}
ratio <- as.integer(n_sim/n_draws)
ratio
```

The ratio should be at least 20 as recommended in Talts et. al (2018). Here we will first replicate the SBC Histogram found in the original tutorial, however we first need to calculate the rank for the data. This will require matrix manipulation, so its important that the dimensions match up.  

```{r}
dim(posterior_samples)
dim(test_sims$parameters)
```

Notice that the posterior samples and the parameters have seemingly similar dimensions, except that the posterior samples has 3 dimensions with the second being 100 long. As we will need to compare the two for rank, it's best to reshape the posterior samples 

```{r}
posterior_samples <- aperm(posterior_samples, c(1,3,2))
dim(posterior_samples)
dim(test_sims$parameters)
```

Now the posterior samples dimensions represent number of sims, number of parameters, and number of draws respectively. Then do compare the parameters with their respective simulations, we want to repeat the parameters for each draw 

```{r}
test_sim_param <- array(test_sims$parameters, c(500,4,100))
dim(test_sim_param)
```

Finally we can compute the rank
```{r}
mask = posterior_samples < test_sim_param
rank = apply(mask, c(1, 2), sum)
```

Next we can move on to visualization. Using ggplot 2 we need to use a dataframe. 

```{r}
rankdf <- as.data.frame(rank)
rankdf <- setNames(rankdf, c("Theta 1", "Theta 2", "Theta 3", "Theta 4"))
head(rankdf)
```
Notice this is a wide dataframe but instead we want a long dataframe

```{r}
library(tidyr)
rankdf <-  pivot_longer(rankdf, cols = c("Theta 1", "Theta 2", "Theta 3", "Theta 4"), names_to = "theta", values_to = "rank")
```

Since we can model the rank distribution witha binomial model, and we expect the rank to be roughly uniformly distributed, we set the bounds of the SBC histogram to be the 99% CI of a binomial distribution, i.e
the bounds should be roughly, 
 $$ n * \Bigg( p \pm z \sqrt{\frac{p(1-p)}{n}} \Bigg) $$
where $z = 2.58$ again this is the normal approximation interval or wald interval 
```{r}
# Using a 0.99 CI for the interval of the graph 
# Consider that our  p = 1/ num_bins 
# p = 1/num_bins
p = 0.1 
N = dim(rank)[1]

low = p - 2.58 * sqrt((p*(1-p))/N)
high = p + 2.58 * sqrt((p*(1-p))/N)

lowerbound = floor(low * N)
upperbound = ceiling(high * N) 
mean = N*p #(binomial mean)
```

Now we can put all of the pieces together. 

```{r}
ggplot(rankdf, aes(x = rank)) + 
  geom_histogram(bins = 10, fill = "maroon4", color = "black") + 
  xlim(lowerbound,upperbound) + 
  xlab("Rank Statistic") + 
  facet_wrap(~theta, nrow=1)
```

Although not included here, if the ratio of simulations-to-posterior-samples is to low for r4easonable density estimation and confidence intervals, you may want to considering using a more modern version of SBC which is based on the empirical cumulative distribution function  (ECDF) and does not have a `num_bins` hyperparameter. You can read more about this method [here](https://arxiv.org/abs/2103.10522).

#### 1.6.3.3 Posterior z-score and contraction

- Posterior z-score: It measures how many standard deviations away the mean of the posterior distribution is from the true value of the parameter. A z-score of 0 indicates the mean perfectly aligns with the true value (no bias) while positive/negative z-scores indicate positive/negative bias, respectively.

- Posterior contraction: It measures how much the posterior distribution contracts around the true value of the parameter as more data is observed. A higher contraction indicates that the data provides strong evidence, narrowing the uncertainty range.


Ideally, we should obtain high contraction and a z-score near 0. This means the model accurately captures the true value with little bias and high confidence.


A quick and dirty way to gain an understanding of how good point estimates and uncertainty estimates capture the “true” parameters, assuming the generative model is well-specified. For this, we will draw more samples from the posteriors in order to get smaller Monte Carlo error.

```{r}
post_samples <- amortizer$sample(test_sims, n_samples = as.integer(1000))
```

Note the shape of our resulting array is `(500, 1000,4)`. As before we have the dimensions represented as 

```{r}
n_sim <- dim(post_samples)[1]
n_draws <- dim(post_samples)[2]
n_params <- dim(post_samples)[3]
dim(post_samples)
```
 We can then look at how the amortizer performed in terms of recovery by examining a scattor plot of the posterior samples against the ground truth. 
 
```{r}
# Create directly comparable dataframes
# dim(post_samples)
# dim(test_sims$parameters)


# Here we want to replicate the input parameter n_draws number of times, which is achieved with the array function and then we want the resulting array be on the dimensions (n_sim, n_draws,n_params) which is achieved with the aperm function

sim_params = aperm( array(test_sims$parameters, c(n_sim, n_params, n_draws)), c(1,3,2) )

# Sanity Check, this should be (500,1000, 4) or (n_sims, n_draws, n_params) in other contexts.
dim(sim_params)

# Reshape the array into a dataframe of the proper form for ggplot
temp_x <- as.data.frame(array_reshape(sim_params, c(n_sim* n_draws, n_params )))

temp_y <- as.data.frame(array_reshape(post_samples, c(n_sim* n_draws, n_params )))

head(temp_x)
head(temp_y)

```
Notice that currently the dataframe is in a wide format and as before we need to convert it to a long format.

```{r}
# Rename Columns
temp_x <- setNames(temp_x, c("Theta 1", "Theta 2", "Theta 3", "Theta 4"))

temp_y <- setNames(temp_y, c("Theta 1", "Theta 2", "Theta 3", "Theta 4"))

# Convert post_samples to long format
temp_y2 <- pivot_longer(temp_y, cols = c("Theta 1", "Theta 2", "Theta 3", "Theta 4"), names_to = "theta", values_to = "y_val")

#Create df for scatterplot 
scatter_df <- pivot_longer(temp_x, cols = c("Theta 1", "Theta 2", "Theta 3", "Theta 4"), names_to = "theta", values_to = "x_val")

scatter_df$y_val <- temp_y2$y_val


```

Finally we can compare the recovered posterior samples to their simualted ground truth. Note also that we need the library `ggpubr` to calculate and display the Pearson correlation coefficent $R$ and $R^2$. 

```{r}
library(ggpubr)

ggplot(scatter_df, mapping = aes(x= x_val, y=y_val))+ geom_point(color = "maroon4", alpha=0.05) + geom_abline(slope = 1, intercept = c(0,0),linetype = "dashed" )+
  stat_cor( aes(label = ..rr.label..), label.y = 3.5, digits =3) + 
  stat_cor( aes(label = c( ..r.label..)),label.y = 2, digits =3) + xlab("Ground Truth") + ylab("Estimated")+ facet_wrap(~theta)  
```

Even better, you might want to inspect the sensitivity of the model in terms of how good some expectation (e.g., the mean) captures the ground truth parameter and how much the posterior shrinks with regard to the prior (i.e., so called posterior contraction). For that, we can compute the prior variance analytically or simply estimate it via Monte Carlo. Using the following definitions we can compute the posterio z-scores and posterior contraction

$$post\_z\_score = \frac{(posterior\_mean - true\_parameters)}{ posterior\_std }$$
and 
$$ post\_contraction = 1 - \Bigg( \frac{posterior\_variance}{ prior\_variance} \Bigg)
 $$ 

```{r}
library(dplyr)

contraction <- scatter_df %>% 
  mutate(prior_var= var(x_val)) %>%  # calculate prior_variance
  group_by(theta, x_val) %>% # group by the generating parameters 
  mutate( post_var = var(y_val)) %>% # calculate the posterior variance for each simulatated parameter 
  mutate(post_contraction = 1 - post_var/prior_var, z_score= scale(y_val)) # calculate contractiuon and zscore (scale)


ggplot(contraction, mapping = aes(x = post_contraction, y = z_score)) + 
  geom_point(color = "maroon4", alpha = 0.05) + 
  xlim(0,1)+ 
  facet_wrap(~theta) + 
  xlab("Posterior Contraction") + 
  ylab("Posterior z-score") 
```

We observe the best case of model adequacy - no bias and large contraction. You can play around with different samples sizes per simulation (i.e., the n_obs argument in the likelihood function) and check all diagnostics again. Or, even better, you can try estimating your own models!

## 1.7 Inference Phase 

Once the approximator has passed all consistency checks, we can now go ahead and apply it to real data! Since the data-generating parameters of real systems are per definition unobservable, we cannot use the same methods as below for ascertaining real-world validity of our inferences. Hence, as in any modeling scenario, we would need external validation and posterior predictive checks.

```{r}
# Your code here 
```


## 1.8 Reference 

As an added bonus below is helpful code to copy and paste at your convience. 

```{r}
sessionInfo()
```

Here is a working example of the entire training process

```{r eval = FALSE}

# Define the prior
prior_fun <- function(D=4){
  mu <- rep(0,D)
  Sigma <- diag(D)
  return(mvrnorm(1,mu,Sigma))
}

# Connect BayesFlow to prior function
prior <- bf$simulation$Prior(prior_fun = r_to_py(prior_fun))

# Define the Likelihood function 
likelihood_fun <- function(params, n_obs = 50){
  
  mu <- as.vector(py_to_r(params)) 
  D <- length(mu)
  Sigma <- diag(D)
  
  return(mvrnorm(n=n_obs, mu=mu, Sigma = Sigma )) 
}

# Pass likelihood function to BayesFlow
simulator <- bf$simulation$Simulator(simulator_fun = r_to_py(likelihood_fun))

#Define the Model using the Prior and Likelihood functions
model <- bf$simulation$GenerativeModel(prior = prior, simulator = simulator)

# Generate Data
out <- model(batch_size = as.integer(3))

# Define the Summary network
summary_net <- bf$networks$DeepSet(summary_dim=as.integer(10))

# Create Test Input & a Summary Representation
test_input <- model(batch_size = as.integer(3))
summary_rep <- summary_net(r_to_py(test_input$sim_data))


# Define an Inference Network
inference_net <- bf$networks$InvertibleNetwork(
  num_params = as.integer(4),
  num_coupling_layers = as.integer(4), 
  coupling_settings = r_to_py ( 
    list("dense_args"= list("kernel_regularizer" = NULL), "dropout" = FALSE))
)

# Use inference network on test input
cINN_outputs <- inference_net(r_to_py(test_input$prior_draws),summary_rep)

# Define the Amortizer
amortizer <- bf$amortizers$AmortizedPosterior(inference_net, summary_net)

# Define the Trainer
trainer <- bf$trainers$Trainer(amortizer = amortizer,generative_model = model)

## Training Phase ##
history <- trainer$train_online(epochs = as.integer(10), iterations_per_epoch = as.integer(1000), batch_size = as.integer(32), validation_sims = as.integer(200))

# Final step is to look at diagnostics

```





